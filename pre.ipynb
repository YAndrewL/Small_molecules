{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing for feature extraction\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import esm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protbert\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False, cache_dir='./cache_model/')  # specify the cache model path\n",
    "pretrain_model = BertModel.from_pretrained(\"Rostlab/prot_bert\" , cache_dir='./cache_model/')\n",
    "# utilization \n",
    "\n",
    "def get_protein_features(seq):\n",
    "    sequence_Example = ' '.join(seq)\n",
    "    sequence_Example = re.sub(r\"[UZOB]\", \"X\", sequence_Example)\n",
    "    encoded_input = tokenizer(sequence_Example, return_tensors='pt')\n",
    "    last_hidden = pretrain_model(**encoded_input).last_hidden_state.squeeze(0)[1:-1,:]\n",
    "    return last_hidden.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pkl_generator(root, saver):\n",
    "    data = open(root, 'r').readlines()\n",
    "    data_dict = {}\n",
    "    for i in tqdm(range(len(data))):\n",
    "        if data[i].startswith('>'):\n",
    "            seq = data[i+1].strip()\n",
    "            label = data[i+2].strip()\n",
    "            data_dict[data[i].strip()[1:]] = (get_protein_features(seq), label)\n",
    "    pickle.dump(data_dict, open(saver, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['./Raw_data/scpdb.txt']\n",
    "savers = ['./Dataset/scpdb.pkl']\n",
    "for file, saver in zip(files, savers):\n",
    "    data_pkl_generator(file, saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esm-v2\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()\n",
    "\n",
    "raws = pickle.load(open('scpdb.pkl', 'rb'))  # generated protbert file\n",
    "\n",
    "# data training data\n",
    "data_dict = {}\n",
    "data = open(\"joined.txt\",'r').readlines()\n",
    "for i in tqdm(range(len(data))):\n",
    "    if data[i].startswith('>'):\n",
    "        pid = data[i].strip()[1:]\n",
    "        seq = [(pid, data[i+1].strip())]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(seq)\n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "        token_representations = results[\"representations\"][33]\n",
    "        data_dict[pid] = (token_representations.squeeze(0)[1:-1, :], raws[pid][1])\n",
    "\n",
    "pickle.dump(data_dict, open(\"esm_scpdb.pkl\", 'wb'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
